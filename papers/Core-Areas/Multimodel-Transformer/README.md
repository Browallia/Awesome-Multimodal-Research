# Multimodel Transformer

[Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling](https://arxiv.org/pdf/2102.06183.pdf), CVPR 2021 [[code]](https://github.com/jayleicn/ClipBERT)

[ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/pdf/2102.03334.pdf) ICML 2021[[code]](https://github.com/dandelin/ViLT)

[Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks](https://arxiv.org/pdf/2004.06165.pdf), ECCV 2020 [[code]](https://github.com/microsoft/Oscar)

[UNITER: UNiversal Image-TExt Representation Learning](https://arxiv.org/abs/1909.11740), ECCV 2020 [[code]](https://github.com/ChenRocks/UNITER)

[LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/pdf/1908.07490.pdf), EMNLP 2019 [[code]](https://github.com/airsplay/lxmert)

[Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers](https://arxiv.org/pdf/2004.00849.pdf) 

[UniT: Multimodal Multitask Learning with a Unified ](https://arxiv.org/pdf/2102.10772.pdf) [[code]](https://mmf.sh/docs/projects/unit/)

[VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text](https://arxiv.org/pdf/2104.11178.pdf) 

[ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](https://arxiv.org/abs/1908.02265), NeurIPS 2019 [[code]](https://github.com/jiasenlu/vilbert_beta)

[VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/abs/1908.03557), arXiv 2019 [[code\]](https://github.com/uclanlp/visualbert)

